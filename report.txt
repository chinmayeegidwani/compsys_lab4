Chinmayee Gidwani
1003062473
Q1. If we do not #ifdef out unused methods and data structures, they could impact performance and produce conflicts in the program.


Q2. Using TM was easier than implementing the global lock above. With TM, we only have to wrap the critical section
 with the __transaction_atomic{} tag, whereas with the global lock, it required setting up and initializing the lock.

Q3. Yes, this is possible. It can be done by having a lock for each list in the array. Then, when we run the program, 
place the lock of the list we are trying to access around the lookup, insert, and count increment block.

Q4. No, because lookup, insert, and the count increment has to be atomic. Otherwise, the table could be modified between lookup and insert. 
Similarly, the table could be modified between insert and the count increment, which would cause the two to be out of sync. 

Q5. Yes, this is possible. Lookup, insert, and the count increment can be atomic within this function, which would ensure correctness.

Q6. Yes, this is possible. This would be similar to the first solution, except implemented inside the class. 

Q7. Using TM was easier than implementing the list locking. With TM, we only have to place the tag around the critical section,
 whereas with list locking, we have to implement a new mutex lock for each list, which requires more effort.

Q8. The pros are that we no longer require any locking mechanism, since there can be no conflicts with private tables. This makes 
implementation easier, and might make the program faster, since there will be no contention or waiting on other threads. The cons are 
that it has poor memory utilization, which impacts program performance. It also carries the overhead of synching the tables after the 
threads have completed running.

Q9. Samples to skip = 50: The overhead of parallelization is 1.01, 1.055, 1.034, and 1.06 for transactional memory, list lock, 
element lock, and reduction respectively.
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Num Threads/Locking Scheme    | 1        | 2        | 4       |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Original                      | 11.20    | N/A      | N/A     |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Global_lock                   | 11.28    | 7.39     | 7.10    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Tm                            | 11.42    | 11.36    | 7.71    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| List_lock                     | 11.91    | 6.27     | 3.58    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Element_lock                  | 11.67    | 6.37     | 3.72    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Reduction                     | 11.90    | 6.61     | 3.51    |
+-------------------------------+----------+----------+---------+

Q10. The performance of the program improves as number of threads increases. This is expected, since more threads will reduce each threads workload.

Q11. Samples to skip = 100: Overall, the times all go up during these experiments. This is because the loop that calls rand_r runs 100 times instead
 of 50, which approximately doubles the run time.

+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Num Threads/Locking Scheme    | 1        | 2        | 4       |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Original                      | 21.79    | N/A      | N/A     |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Global_lock                   | 21.98    | 12.14    | 7.99    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Tm                            | 22.77    | 16.44    | 9.81    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| List_lock                     | 22.65    | 11.52    | 7.05    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Element_lock                  | 23.14    | 12.15    | 6.89    |
+-------------------------------+----------+----------+---------+
|                               |          |          |         |
| Reduction                     | 21.22    | 10.67    | 6.01    |
+-------------------------------+----------+----------+---------+

Q12. Assuming that all the enums remain the same (ie RAND_NUM_UPPER_BOUND and NUM_SEED_STREAMS), OptsrUs should use the list lock. Using process of elimination, 
global lock and transactional memory perform significantly worse than list lock or element lock. So looking at just the latter two schemes, the element lock performs 
marginally better. However, it would also require significantly more memory. Similarly, the reduction option uses 4 times the memory as any of the other options, which
 is not desirable. Thus, the list lock has a good balance of high performance and memory utilization.
